{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2b8a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import riva.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b5ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76b073c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import io\n",
    "import IPython.display as ipd\n",
    "import wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1724b456",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = riva.client.Auth(uri='localhost:50051')\n",
    "riva_asr = riva.client.ASRService(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e57df474",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_FILE = 'harvard.wav'\n",
    "with wave.open(AUDIO_FILE, \"rb\") as wav:\n",
    "    audio = wav.readframes(wav.getnframes())\n",
    "# ipd.Audio(audio, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "badd3f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = riva.client.RecognitionConfig(\n",
    "    encoding=riva.client.AudioEncoding.LINEAR_PCM,\n",
    "    sample_rate_hertz=16000,\n",
    "    language_code=\"vi-VN\",  \n",
    "    max_alternatives=1,\n",
    "    enable_automatic_punctuation=True,\n",
    "    audio_channel_count = 1   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b287d390",
   "metadata": {},
   "outputs": [
    {
     "ename": "_InactiveRpcError",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"Error: Unavailable model requested given these parameters: language_code=vi; sample_rate=16000; type=offline; \"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B::1%5D:50051 {created_time:\"2025-11-12T19:16:38.3790272+00:00\", grpc_status:3, grpc_message:\"Error: Unavailable model requested given these parameters: language_code=vi; sample_rate=16000; type=offline; \"}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m riva_asr\u001b[38;5;241m.\u001b[39moffline_recognize(audio, config)\n\u001b[0;32m      2\u001b[0m asr_best_transcript \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mresults[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39malternatives[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtranscript\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASR Transcript:\u001b[39m\u001b[38;5;124m\"\u001b[39m, asr_best_transcript)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\riva\\client\\asr.py:485\u001b[0m, in \u001b[0;36mASRService.offline_recognize\u001b[1;34m(self, audio_bytes, config, future)\u001b[0m\n\u001b[0;32m    483\u001b[0m request \u001b[38;5;241m=\u001b[39m rasr\u001b[38;5;241m.\u001b[39mRecognizeRequest(config\u001b[38;5;241m=\u001b[39mconfig, audio\u001b[38;5;241m=\u001b[39maudio_bytes)\n\u001b[0;32m    484\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstub\u001b[38;5;241m.\u001b[39mRecognize\u001b[38;5;241m.\u001b[39mfuture \u001b[38;5;28;01mif\u001b[39;00m future \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstub\u001b[38;5;241m.\u001b[39mRecognize\n\u001b[1;32m--> 485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(request, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth\u001b[38;5;241m.\u001b[39mget_auth_metadata())\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\grpc\\_channel.py:1181\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1168\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1173\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1174\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1175\u001b[0m     (\n\u001b[0;32m   1176\u001b[0m         state,\n\u001b[0;32m   1177\u001b[0m         call,\n\u001b[0;32m   1178\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[0;32m   1179\u001b[0m         request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[0;32m   1180\u001b[0m     )\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\grpc\\_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[1;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[0;32m   1004\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m state\u001b[38;5;241m.\u001b[39mresponse\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[1;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"Error: Unavailable model requested given these parameters: language_code=vi; sample_rate=16000; type=offline; \"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B::1%5D:50051 {created_time:\"2025-11-12T19:16:38.3790272+00:00\", grpc_status:3, grpc_message:\"Error: Unavailable model requested given these parameters: language_code=vi; sample_rate=16000; type=offline; \"}\"\n>"
     ]
    }
   ],
   "source": [
    "response = riva_asr.offline_recognize(audio, config)\n",
    "asr_best_transcript = response.results[0].alternatives[0].transcript\n",
    "print(\"ASR Transcript:\", asr_best_transcript)\n",
    "\n",
    "print(\"\\n\\nFull Response Message:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7609662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è  K·∫øt qu·∫£ nh·∫≠n d·∫°ng:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ASRService.streaming_response_generator() got an unexpected keyword argument 'end_of_stream'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m audio_bytes[i:i\u001b[38;5;241m+\u001b[39mchunk_size]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# G·ªçi inference\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m responses \u001b[38;5;241m=\u001b[39m riva_asr\u001b[38;5;241m.\u001b[39mstreaming_response_generator(\n\u001b[0;32m     34\u001b[0m     audio_chunks(audio_bytes),\n\u001b[0;32m     35\u001b[0m     config,\n\u001b[0;32m     36\u001b[0m     end_of_stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# ‚ö†Ô∏è b√°o audio ƒë√£ h·∫øt\u001b[39;00m\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müó£Ô∏è  K·∫øt qu·∫£ nh·∫≠n d·∫°ng:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m responses:\n",
      "\u001b[1;31mTypeError\u001b[0m: ASRService.streaming_response_generator() got an unexpected keyword argument 'end_of_stream'"
     ]
    }
   ],
   "source": [
    "import riva.client\n",
    "from riva.client import ASRService\n",
    "import wave\n",
    "\n",
    "# ƒê·ªãa ch·ªâ server\n",
    "RIVA_SERVER = \"localhost:50051\"\n",
    "\n",
    "# K·∫øt n·ªëi Riva\n",
    "auth = riva.client.Auth(uri=RIVA_SERVER)\n",
    "riva_asr = ASRService(auth)\n",
    "\n",
    "# File √¢m thanh test (16kHz, mono, wav)\n",
    "AUDIO_FILE = \"harvard.wav\"\n",
    "\n",
    "with wave.open(AUDIO_FILE, \"rb\") as wav:\n",
    "    audio_bytes = wav.readframes(wav.getnframes())\n",
    "\n",
    "# C·∫•u h√¨nh ASR\n",
    "config = riva.client.RecognitionConfig(\n",
    "    encoding=riva.client.AudioEncoding.LINEAR_PCM,\n",
    "    sample_rate_hertz=16000,\n",
    "    language_code=\"vi-VN\",\n",
    "    max_alternatives=1,\n",
    "    enable_automatic_punctuation=True,\n",
    "    verbatim_transcripts=False\n",
    ")\n",
    "print(\"üó£Ô∏è  K·∫øt qu·∫£ nh·∫≠n d·∫°ng:\")\n",
    "def audio_chunks(audio_bytes, chunk_size=4000):\n",
    "    for i in range(0, len(audio_bytes), chunk_size):\n",
    "        yield audio_bytes[i:i+chunk_size]\n",
    "\n",
    "# G·ªçi inference\n",
    "responses = riva_asr.streaming_response_generator(\n",
    "    audio_chunks(audio_bytes),\n",
    "    config,\n",
    "    end_of_stream=True  # ‚ö†Ô∏è b√°o audio ƒë√£ h·∫øt\n",
    ")\n",
    "print(\"üó£Ô∏è  K·∫øt qu·∫£ nh·∫≠n d·∫°ng:\")\n",
    "for response in responses:\n",
    "    for result in response.results:\n",
    "        if len(result.alternatives) > 0:\n",
    "            print(\"Transcript:\", result.alternatives[0].transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c4e476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è  K·∫øt qu·∫£ nh·∫≠n d·∫°ng:\n"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "import riva.client\n",
    "from riva.client import ASRService\n",
    "\n",
    "# K·∫øt n·ªëi server\n",
    "RIVA_SERVER = \"localhost:50051\"\n",
    "auth = riva.client.Auth(uri=RIVA_SERVER)\n",
    "riva_asr = ASRService(auth)\n",
    "\n",
    "# ƒê·ªçc file WAV 16kHz mono\n",
    "AUDIO_FILE = \"harvard.wav\"\n",
    "with wave.open(AUDIO_FILE, \"rb\") as wav:\n",
    "    audio_bytes = wav.readframes(wav.getnframes())\n",
    "\n",
    "# Generator chia chunk v√† b√°o end-of-stream\n",
    "def audio_chunks(audio_bytes, chunk_size=4000):\n",
    "    for i in range(0, len(audio_bytes), chunk_size):\n",
    "        yield audio_bytes[i:i+chunk_size]\n",
    "    yield b\"\"  # ‚ö†Ô∏è k·∫øt th√∫c streaming\n",
    "\n",
    "# C·∫•u h√¨nh ASR\n",
    "config = riva.client.RecognitionConfig(\n",
    "    encoding=riva.client.AudioEncoding.LINEAR_PCM,\n",
    "    sample_rate_hertz=16000,\n",
    "    language_code=\"vi-VN\",\n",
    "    max_alternatives=1,\n",
    "    enable_automatic_punctuation=True,\n",
    "    verbatim_transcripts=False\n",
    ")\n",
    "\n",
    "# G·ªçi streaming inference\n",
    "responses = riva_asr.streaming_response_generator(audio_chunks(audio_bytes), config)\n",
    "\n",
    "print(\"üó£Ô∏è  K·∫øt qu·∫£ nh·∫≠n d·∫°ng:\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ae529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ASRService.streaming_response_generator at 0x0000022A94CD2110>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90cba176",
   "metadata": {},
   "outputs": [
    {
     "ename": "_MultiThreadedRendezvous",
     "evalue": "<_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNKNOWN\n\tdetails = \"Exception iterating requests!\"\n\tdebug_error_string = \"None\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_MultiThreadedRendezvous\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 27\u001b[0m\n\u001b[0;32m     16\u001b[0m config \u001b[38;5;241m=\u001b[39m RecognitionConfig(\n\u001b[0;32m     17\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mAudioEncoding\u001b[38;5;241m.\u001b[39mLINEAR_PCM,\n\u001b[0;32m     18\u001b[0m     sample_rate_hertz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     verbatim_transcripts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     25\u001b[0m responses \u001b[38;5;241m=\u001b[39m riva_asr\u001b[38;5;241m.\u001b[39mstreaming_response_generator(audio_chunks(audio_bytes), config)\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m responses:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mresults:\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m alt \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39malternatives:\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\riva\\client\\asr.py:443\u001b[0m, in \u001b[0;36mASRService.streaming_response_generator\u001b[1;34m(self, audio_chunks, streaming_config)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;124;03mGenerates speech recognition responses for fragments of speech audio in :param:`audio_chunks`.\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;124;03mThe purpose of the method is to perform speech recognition \"online\" - as soon as\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;124;03m    <https://docs.nvidia.com/deeplearning/riva/user-guide/docs/reference/protos/protos.html#riva-proto-riva-asr-proto>`_.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    442\u001b[0m generator \u001b[38;5;241m=\u001b[39m streaming_request_generator(audio_chunks, streaming_config)\n\u001b[1;32m--> 443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstub\u001b[38;5;241m.\u001b[39mStreamingRecognize(generator, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth\u001b[38;5;241m.\u001b[39mget_auth_metadata()):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\grpc\\_channel.py:543\u001b[0m, in \u001b[0;36m_Rendezvous.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next()\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\grpc\\_channel.py:952\u001b[0m, in \u001b[0;36m_MultiThreadedRendezvous._next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 952\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_response_ready\u001b[39m():\n\u001b[0;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    956\u001b[0m         cygrpc\u001b[38;5;241m.\u001b[39mOperationType\u001b[38;5;241m.\u001b[39mreceive_message \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mdue\n\u001b[0;32m    957\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    958\u001b[0m     )\n",
      "\u001b[1;31m_MultiThreadedRendezvous\u001b[0m: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNKNOWN\n\tdetails = \"Exception iterating requests!\"\n\tdebug_error_string = \"None\"\n>"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "from riva.client import ASRService, RecognitionConfig, AudioEncoding, Auth\n",
    "\n",
    "auth = Auth(uri=\"localhost:50051\")\n",
    "riva_asr = ASRService(auth)\n",
    "\n",
    "AUDIO_FILE = \"output_16k.wav\"\n",
    "with wave.open(AUDIO_FILE, \"rb\") as wav:\n",
    "    audio_bytes = wav.readframes(wav.getnframes())\n",
    "\n",
    "def audio_chunks(audio_bytes, chunk_size=4000):\n",
    "    for i in range(0, len(audio_bytes), chunk_size):\n",
    "        yield audio_bytes[i:i+chunk_size]\n",
    "    yield b\"\"  # b√°o k·∫øt th√∫c stream\n",
    "\n",
    "config = RecognitionConfig(\n",
    "    encoding=AudioEncoding.LINEAR_PCM,\n",
    "    sample_rate_hertz=16000,\n",
    "    language_code=\"vi-VN\",\n",
    "    max_alternatives=1,\n",
    "    enable_automatic_punctuation=True,\n",
    "    verbatim_transcripts=False\n",
    ")\n",
    "\n",
    "responses = riva_asr.streaming_response_generator(audio_chunks(audio_bytes), config)\n",
    "\n",
    "for response in responses:\n",
    "    for result in response.results:\n",
    "        for alt in result.alternatives:\n",
    "            transcript = alt.transcript\n",
    "            speaker = getattr(alt, \"speaker_tag\", None)\n",
    "            if speaker:\n",
    "                print(f\"[Speaker {speaker}]: {transcript}\")\n",
    "            else:\n",
    "                print(f\"[Transcript]: {transcript}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b5fae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "d:\\Anaconda\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "d:\\Anaconda\\Lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# ƒê·ªçc file g·ªëc\n",
    "y, sr = librosa.load(\"harvard.wav\", sr=None)  # sr=None ƒë·ªÉ gi·ªØ nguy√™n sample rate g·ªëc\n",
    "\n",
    "# Resample sang 16kHz\n",
    "y_16k = librosa.resample(y, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "# Ghi l·∫°i\n",
    "sf.write(\"output_16k.wav\", y_16k, 16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3785837",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'multi_speaker.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 2Ô∏è‚É£ File audio multi-speaker\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[0;32m     14\u001b[0m AUDIO_FILE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_speaker.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# WAV 16kHz mono, nhi·ªÅu gi·ªçng\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wave\u001b[38;5;241m.\u001b[39mopen(AUDIO_FILE, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m wav:\n\u001b[0;32m     16\u001b[0m     audio_bytes \u001b[38;5;241m=\u001b[39m wav\u001b[38;5;241m.\u001b[39mreadframes(wav\u001b[38;5;241m.\u001b[39mgetnframes())\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 3Ô∏è‚É£ Generator chia chunk v√† b√°o end-of-stream\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\wave.py:649\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(f, mode)\u001b[0m\n\u001b[0;32m    647\u001b[0m         mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Wave_read(f)\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Wave_write(f)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\wave.py:282\u001b[0m, in \u001b[0;36mWave_read.__init__\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i_opened_the_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 282\u001b[0m     f \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i_opened_the_file \u001b[38;5;241m=\u001b[39m f\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# else, assume it is an open file object already\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'multi_speaker.wav'"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "from riva.client import ASRService, RecognitionConfig, AudioEncoding, Auth\n",
    "\n",
    "# =========================\n",
    "# 1Ô∏è‚É£ K·∫øt n·ªëi Riva Server\n",
    "# =========================\n",
    "RIVA_SERVER = \"localhost:50051\"\n",
    "auth = Auth(uri=RIVA_SERVER)\n",
    "riva_asr = ASRService(auth)\n",
    "\n",
    "# =========================\n",
    "# 2Ô∏è‚É£ File audio multi-speaker\n",
    "# =========================\n",
    "AUDIO_FILE = \"multi_speaker.wav\"  # WAV 16kHz mono, nhi·ªÅu gi·ªçng\n",
    "with wave.open(AUDIO_FILE, \"rb\") as wav:\n",
    "    audio_bytes = wav.readframes(wav.getnframes())\n",
    "\n",
    "# =========================\n",
    "# 3Ô∏è‚É£ Generator chia chunk v√† b√°o end-of-stream\n",
    "# =========================\n",
    "def audio_chunks(audio_bytes, chunk_size=4000):\n",
    "    for i in range(0, len(audio_bytes), chunk_size):\n",
    "        yield audio_bytes[i:i+chunk_size]\n",
    "    yield b\"\"  # ‚ö†Ô∏è b√°o audio k·∫øt th√∫c\n",
    "\n",
    "# =========================\n",
    "# 4Ô∏è‚É£ C·∫•u h√¨nh ASR + Diarizer\n",
    "# =========================\n",
    "config = RecognitionConfig(\n",
    "    encoding=AudioEncoding.LINEAR_PCM,\n",
    "    sample_rate_hz=16000,\n",
    "    language_code=\"vi-VN\",\n",
    "    max_alternatives=1,\n",
    "    enable_automatic_punctuation=True,\n",
    "    verbatim_transcripts=False\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 5Ô∏è‚É£ Streaming inference\n",
    "# =========================\n",
    "responses = riva_asr.streaming_response_generator(\n",
    "    audio_chunks(audio_bytes),\n",
    "    config\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 6Ô∏è‚É£ Hi·ªÉn th·ªã transcript c√≥ t√°ch gi·ªçng\n",
    "# =========================\n",
    "print(\"üó£Ô∏è K·∫øt qu·∫£ nh·∫≠n d·∫°ng v·ªõi t√°ch gi·ªçng:\")\n",
    "for response in responses:\n",
    "    for result in response.results:\n",
    "        for alt in result.alternatives:\n",
    "            transcript = alt.transcript\n",
    "            speaker = getattr(alt, \"speaker_tag\", None)\n",
    "            if speaker is not None:\n",
    "                print(f\"[Speaker {speaker}]: {transcript}\")\n",
    "            else:\n",
    "                print(f\"[Transcript]: {transcript}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71b7b518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 16000\n"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "with wave.open(\"output_16k.wav\", \"rb\") as wav:\n",
    "    print(wav.getnchannels(), wav.getsampwidth(), wav.getframerate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ddf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "from riva.client import ASRService, RecognitionConfig, AudioEncoding, Auth\n",
    "\n",
    "# K·∫øt n·ªëi server\n",
    "RIVA_SERVER = \"localhost:50051\"\n",
    "auth = Auth(uri=RIVA_SERVER)\n",
    "riva_asr = ASRService(auth)\n",
    "\n",
    "# ƒê·ªçc audio 16kHz, mono, PCM16\n",
    "AUDIO_FILE = \"output_16k.wav\"\n",
    "with wave.open(AUDIO_FILE, \"rb\") as wav:\n",
    "    audio_bytes = wav.readframes(wav.getnframes())\n",
    "\n",
    "# C·∫•u h√¨nh ASR\n",
    "config = RecognitionConfig(\n",
    "    encoding=AudioEncoding.LINEAR_PCM,\n",
    "    sample_rate_hz=16000,\n",
    "    language_code=\"vi-VN\",\n",
    "    max_alternatives=1,\n",
    "    enable_automatic_punctuation=True,\n",
    "    verbatim_transcripts=False\n",
    ")\n",
    "\n",
    "# G·ªçi model offline\n",
    "response = riva_asr.offline_recognize(audio_bytes, config)\n",
    "\n",
    "# In k·∫øt qu·∫£\n",
    "for result in response.results:\n",
    "    for alt in result.alternatives:\n",
    "        transcript = alt.transcript\n",
    "        speaker = getattr(alt, \"speaker_tag\", None)\n",
    "        if speaker is not None:\n",
    "            print(f\"[Speaker {speaker}]: {transcript}\")\n",
    "        else:\n",
    "            print(f\"[Transcript]: {transcript}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
